
<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Daksh Thapar


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22></text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="assets/js/theme.js"></script>
<script src="assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          <!--
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>-->
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     Daksh Thapar
    </h1>
     <p class="desc"><strong>PhD Student. </strong> School of Computing and Electrical Engineering, IIT Mandi</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded-circle" src="./assets/img/my_img.jpeg">
      
      
    </div>
    

    <div class="clearfix">
      <p>I am a PhD Student in the School of Computing and Electrical Engineering at <a href="https://iitmandi.ac.in">Indian Institute of Technology Mandi</a>, Mandi, India, under the supervision of Assistant Professor <a href="https://faculty.iitmandi.ac.in/~aditya">Dr. Aditya Nigam</a> (IIT Mandi) and Associate Professor <a href="https://www.cse.iitd.ac.in/~chetan/">Dr. Chetan Arora</a> (IIT Delhi).</p>

<p>I received a BE in Computer Science and Engineering from the <a href="https://www.puchd.ac.in/">Panjab University, Chandigarh</a>. My current Ph.D Thesis is Identity and Attribute extraction from egocentric and surveillance videos, which I intend to submit by May 2022. Here is <a href="./assets/pdf/My_CV_Daksh.pdf">my resume</a>.</p>

<p><strong> Research Interests:</strong> Computer Vision, Deep Learning, Video Analaysis, Medical Imaging, Natural Language Processing, Speech Precessing</p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      <tr>
          <th scope="row">March 02, 2022</th>
          <td>
            
              Paper acceped to CVPR 2022

            
          </td>
        </tr>
      <tr>
          <th scope="row">Oct 17, 2021</th>
          <td>
            
              Gave a presentation at Ninth International Workshop on Egocentric Perception, Interaction and Computing in ICCV 2021

            
          </td>
        </tr>
        <tr>
          <th scope="row">Jul 22, 2021</th>
          <td>
            
              Paper acceped to ICCV 2021 as oral presentation (3% of total submissions)

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 3, 2020</th>
          <td>
            
              Paper acceped to ECCV 2020

            
          </td>
        </tr>
      
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications (see Google Scholar for full list)</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICCV</abbr>
    
  
  </div>

  <div id="abs-2108-07884" class="col-sm-8">
    
      <div class="title">Anonymizing Egocentric Videos</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Daksh Thapar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Aditya Nigam,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                and  Chetan Arora
                
              
            
          
        
          
          
          
          
          
          
            
              
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ICCV</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <!-- <a href="https://arxiv.org/abs/2108.07884" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a> -->
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In egocentric videos, the face of a wearer capturing the video is never captured. This gives a false sense of security that the wearer's privacy is preserved while sharing such videos. However, egocentric cameras are typically harnessed to wearer's head, and hence, also capture wearer's gait. Recent works have shown that wearer gait signatures can be extracted from egocentric videos, which can be used to determine if two egocentric videos have the same wearer. In a more damaging scenario, one can even recognize a wearer using hand gestures from egocentric videos, or identify a wearer in third person videos such as from a surveillance camera. We believe, this could be a death knell in sharing of egocentric videos, and fatal for egocentric vision research. In this work, we suggest a novel technique to anonymize egocentric videos, which create carefully crafted, but small, and imperceptible optical flow perturbations in an egocentric video's frames. Importantly, these perturbations do not affect object detection or action/activity recognition from egocentric videos but are strong enough to dis-balance the gait recovery process. In our experiments on benchmark Epic-Kitchens dataset, the proposed perturbation degrades the wearer recognition performance, from 66.3% to 13.4%, while preserving the activity recognition performance  from 89.6% to 87.4%. To test our anonymization with more wearer recognition techniques, we also developed a stronger, and more generalizable wearer recognition method based on camera egomotion cues. The approach achieves state-of-the-art (SOTA) performance of 59.67% on Epic-Kitchens, compared to 55.06%. However, the accuracy of our recognition technique also drops to 12% using the proposed anonymizing perturbations.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">IEEE CEM</abbr>
    
  
  </div>

  <div id="abs-2105-04551" class="col-sm-8">
    
      <div class="title">LakshmanRekha: AI-biometric driven Smartphone App for strict Post-COVID Home Quarantine Management</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Gaurav Jaiswal,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Rohit J. Bharadwaj,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kamlesh Tiwari,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Daksh Thapar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Piyush Goyal,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Aditya Nigam
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In IEEE</em>
      
      Consumer Electronics Magazine
        
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/document/9263327" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <!-- <a href="https://github.com/CompVis/image2video-synthesis-using-cINNs" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a> -->
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>COVID-19 has been announced as a Global Communal Health Extremity by WHO on January 2020. Meaningful preventive solutions have been taken with smartphone selfie/geofencing apps toward managing mandatory home quarantine and physical distancing. In the post-COVID world, fast screening and strict quarantine can play a crucial role in bringing back normality. Quarantine being offered at home can be a comfortable solution for both government and patients. On the other hand, it can be hazardous if not strictly followed and adequately realized. However, the existing geofencing/face selfie apps take static photographs and location data at certain time intervals that can allow patients to violate the rules between those periods, thus failing to ensure active user identity. To realize unbreached home quarantine policies, this article introduces a CUBA-HQM smartphone app that performs continuous user biometric authentication (CUBA) augmented with geofencing using AI technology. The purpose of continuous tracking is to strictly control the spread of infectious diseases in society by monitoring the individual move in/out in the quarantine zone.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ECCV</abbr>
    
  
  </div>

  <div id="abs-2105-05217" class="col-sm-8">
    
      <div class="title">Is sharing of egocentric video giving away your biometric signature?</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Daksh Thapar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Aditya Nigam,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Chetan Arora.
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ECCV</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/2781_ECCV_2020_paper.php" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <a href="https://egocentricbiometric.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Easy availability of wearable egocentric cameras, and the sense of privacy propagated by the fact that the wearer is never seen in the captured videos, has led to a tremendous rise in public sharing of such videos. Unlike hand-held cameras, egocentric cameras are harnessed on the wearer’s head, which makes it possible to track the wearer’s head motion by observing optical flow in the egocentric videos. In this work, we create a novel kind of privacy attack by extracting the wearer’s gait profile, a well known biometric signature, from such optical flow in the egocentric videos. We demonstrate strong wearer recognition capabilities based on extracted gait features, an unprecedented and critical weakness completely absent in hand-held videos. We demonstrate the following attack scenarios: (1) In a closed-set scenario, we show that it is possible to recognize the wearer of an egocentric video with an accuracy of more than 92.5% on the benchmark video dataset. (2) In an open-set setting, when the system has not seen the camera wearer even once during the training, we show that it is still possible to identify that the two egocentric videos have been captured by the same wearer with an Equal Error Rate (EER) of less than 14.35%. (3) We show that it is possible to extract gait signature even if only sparse optical flow and no other scene information from egocentric video is available. We demonstrate the accuracy of more than 84% for wearer recognition with only global optical flow. (4) While the first person to first person matching does not give us access to the wearer’s face, we show that it is possible to match the extracted gait features against the one obtained from a third person view such as a surveillance camera looking at the wearer in a completely different background at a different time. In essence, our work indicates that sharing one’s egocentric video should be treated as giving away one’s biometric identity and recommend much more oversight before sharing of egocentric videos.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACM MM</abbr>
    
  
  </div>

  <div id="afifi" class="col-sm-8">
    
      <div class="title">Recognizing Camera Wearer from Hand Gestures in Egocentric Videos</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Daksh Thapar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Aditya Nigam,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Chetan Arora
                
              
            
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ACM MM</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://dl.acm.org/doi/abs/10.1145/3394171.3413654" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <a href="https://egocentricbiometric.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Wearable egocentric cameras are typically harnessed to a wearer's head, giving them the unique advantage of capturing their points of view. Hoshen and Peleg have shown that egocentric cameras indirectly capture the wearer's gait, which can be used to identify a wearer based on their egocentric videos. The authors have shown a wearer recognition accuracy of up to 77% over 32 subjects. However, an important limitation of their work is that such gait features can be extracted only from walking sequences of a wearer. In this work, we take the privacy threat a notch higher and show that even the wearer's hand gestures, as seen through an egocentric video, leak wearer's identity. We have designed a model to extract and match hand gesture signatures from egocentric videos. We demonstrate the threat on the EPIC kitchen dataset containing 55 hours of the egocentric videos acquired from 32 subjects doing various activities. We show that: (1) Our model can recognize a wearer with an accuracy of up to 73% based on the same activity, i.e., the model has seen 'cut' activity by a wearer in the train set, and recognizes the wearer based on another 'cut' activity by him/her while testing. (2) The hand gesture signatures transfer across activities, i.e., even if our model does not see 'cut' activity of a wearer at the train time, but sees other activities such as 'wash', 'mix' etc., the model can still recognize a wearer with an accuracy of up to 60%, by matching hand gesture signatures of 'cut' at test time with train time signatures of 'wash' or 'mix'. (3) The hand gesture features even transfer across subjects, i.e., even if the model has not seen any activity by some subject, one can still verify a wearer (open-set), and predict that the same wearer has performed both activities with an Equal Error Rate of 15.21%. </p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JASA</abbr>
    
  
  </div>

  <div id="IslamKEJODB21" class="col-sm-8">
    
      <div class="title">Deep metric learning for bioacoustic classification: Overcoming training data scarcity using dynamic triplet loss</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Anshul Thakur,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Daksh Thapar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Padmanabhan Rajan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Aditya Nigam
        
      </div>

      <div class="periodical">
      
        <em>In The Journal of the Acoustical Society of America</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://asa.scitation.org/doi/full/10.1121/1.5118245" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Bioacoustic classification often suffers from the lack of labeled data. This hinders the effective utilization of state-of-the-art deep learning models in bioacoustics. To overcome this problem, the authors propose a deep metric learning-based framework that provides effective classification, even when only a small number of per-class training examples are available. The proposed framework utilizes a multiscale convolutional neural network and the proposed dynamic variant of the triplet loss to learn a transformation space where intra-class separation is minimized and inter-class separation is maximized by a dynamically increasing margin. The process of learning this transformation is known as deep metric learning. The triplet loss analyzes three examples (referred to as a triplet) at a time to perform deep metric learning. The number of possible triplets increases cubically with the dataset size, making triplet loss more suitable than the cross-entropy loss in data-scarce conditions. Experiments on three different publicly available datasets show that the proposed framework performs better than existing bioacoustic classification methods. Experimental results also demonstrate the superiority of dynamic triplet loss over cross-entropy loss in data-scarce conditions. Furthermore, unlike existing bioacoustic classification methods, the proposed framework has been extended to provide open-set classification.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">EMBC</abbr>
    
  
  </div>

  <div id="YuDB20" class="col-sm-8">
    
      <div class="title">Effectiveness of GAN-based Synthetic Samples Generation of Minority Patterns in HEp-2 Cell Images</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Krati Gupta,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Daksh Thapar,
                
              
            
          
        
          
                  Arnav Bhavsar,
          
          
          
          
            
              
                
                  and Anil K. Sao
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In EMBC</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://ieeexplore.ieee.org/abstract/document/9175636" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <!-- <a href="https://github.com/YorkUCVIL/Wavelet-Flow" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a> -->
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>In this paper, we present a framework to address the augmentation of images for the rare and minor appearance of mitotic type staining patterns, for Human Epithelium Type2 (HEp-2) cell images. The identification of mitotic patterns among non-mitotic/interphase patterns is important in the process of diagnosis of various autoimmune disorders. This task leads to a pattern classification problem between mitotic v/s interphase. However, among the two classes, typically, the number of mitotic cells are relatively very less. Thus, in this work, we propose to generate synthetic mitotic samples, which can be used to augment the number of mitotic samples and balance the samples of mitotic and interphase patterns in classification paradigm. An effective feature representation is used, to validate the usefulness of the synthetic samples in classification task, along with a subjective validation done by a medical expert. The results demonstrate that the approach of generating and mingling synthetic samples with existing training data works well and yields good performance, with 0.98 balanced class accuracy (BcA) in one case, over a public dataset, i.e., UQ-SNP I3A Task-3 mitotic cell identification dataset.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ACCV</abbr>
    
  
  </div>

  <div id="DBLP:conf/cvpr/PavlakosZDD17a" class="col-sm-8">
    
      <div class="title">Hierarchical X-Ray Report Generation via Pathology tags and Multi Head Attention</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Preethi Srinivasan,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Daksh Thapar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Arnav Bhavsar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Aditya Nigam
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In ACCV</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Srinivasan_Hierarchical_X-Ray_Report_Generation_via_Pathology_tags_and_Multi_Head_ACCV_2020_paper.html" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
    <a href="https://medicalcaption.github.io" class="btn btn-sm z-depth-0" role="button" target="_blank">Project Page</a>
  
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Examining radiology images, such as X-Ray images as accurately as possible, forms a crucial step in providing the best healthcare facilities. However, this requires high expertise and clinical experience. Even for experienced radiologists, this is a time-consuming task. Hence, the automated generation of accurate radiology reports from chest X-Ray images is gaining popularity. Compared to other image captioning tasks where coherence is the key criterion, medical image captioning requires high accuracy in detecting anomalies and extracting information along with coherence. That is, the report must be easy to read and convey medical facts accurately. We propose a deep neural network to achieve this. Given a set of Chest X-Ray images of the patient, the proposed network predicts the medical tags and generates a readable radiology report. For generating the report and tags, the proposed network learns to extract salient features of the image from a deep CNN and generates tag embeddings for each patient's X-Ray images. We use transformers for learning self and cross attention. We encode the image and tag features with self-attention to get a finer representation. Use both the above features in cross attention with the input sequence to generate the report's Findings. Then, cross attention is applied between the generated Findings and the input sequence to generate the report's Impressions. We use a publicly available dataset to evaluate the proposed network. The performance indicates that we can generate a readable radiology report, with a relatively higher BLEU score over SOTA.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%64%61%6B.%74%68%61%70%61%72@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=-O1uVxIAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>



<a href="https://www.linkedin.com/in/daksh-thapar-45774925/" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/DakshThapar" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>










      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    © Copyright 2021 Daksh Thapar.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
